apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: campaign-finance-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.0.4, pipelines.kubeflow.org/pipeline_compilation_time: '2020-11-01T23:54:53.615244',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "A classification pipeline
      that performs predictions on electoral results.", "inputs": [{"name": "data_path",
      "type": "String"}, {"name": "model_file", "type": "String"}], "name": "Campaign
      finance pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.0.4}
spec:
  entrypoint: campaign-finance-pipeline
  templates:
  - name: campaign-finance-pipeline
    inputs:
      parameters:
      - {name: data_path}
      - {name: model_file}
    dag:
      tasks:
      - {name: create-volume, template: create-volume}
      - name: predict
        template: predict
        dependencies: [create-volume, train]
        arguments:
          parameters:
          - {name: create-volume-name, value: '{{tasks.create-volume.outputs.parameters.create-volume-name}}'}
          - {name: data_path, value: '{{inputs.parameters.data_path}}'}
          - {name: model_file, value: '{{inputs.parameters.model_file}}'}
      - name: print-prediction
        template: print-prediction
        dependencies: [create-volume, predict]
        arguments:
          parameters:
          - {name: create-volume-name, value: '{{tasks.create-volume.outputs.parameters.create-volume-name}}'}
          - {name: data_path, value: '{{inputs.parameters.data_path}}'}
      - name: train
        template: train
        dependencies: [create-volume]
        arguments:
          parameters:
          - {name: create-volume-name, value: '{{tasks.create-volume.outputs.parameters.create-volume-name}}'}
          - {name: data_path, value: '{{inputs.parameters.data_path}}'}
          - {name: model_file, value: '{{inputs.parameters.model_file}}'}
  - name: create-volume
    resource:
      action: create
      manifest: |
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: '{{workflow.name}}-data-volume'
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 1Gi
    outputs:
      parameters:
      - name: create-volume-manifest
        valueFrom: {jsonPath: '{}'}
      - name: create-volume-name
        valueFrom: {jsonPath: '{.metadata.name}'}
      - name: create-volume-size
        valueFrom: {jsonPath: '{.status.capacity.storage}'}
  - name: predict
    container:
      args: [--data-path, '{{inputs.parameters.data_path}}', --model-file, '{{inputs.parameters.model_file}}']
      command:
      - python3
      - -u
      - -c
      - |
        def predict(data_path, model_file):

            import pickle
            import tensorflow as tf
            from tensorflow import keras
            import numpy as np

            # Load the saved Keras model
            classifier = keras.models.load_model(f'{data_path}/{model_file}')

            # Load and unpack the test_data
            with open(f'{data_path}/test_data','rb') as f:
                test_data = pickle.load(f)
            # Separate the X_test from y_test.
            X_test,  y_test = test_data

            # make predictions.
            y_pred = classifier.predict(X_test)

            # create a threshold
            y_pred=(y_pred>0.5)

            with open(f'{data_path}/result.txt', 'w') as result:
                result.write(" Prediction: {}, Actual: {} ".format(y_pred,y_test.astype(np.bool)))

            print('Prediction has be saved successfully!')

        import argparse
        _parser = argparse.ArgumentParser(prog='Predict', description='')
        _parser.add_argument("--data-path", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-file", dest="model_file", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = predict(**_parsed_args)
      image: tensorflow/tensorflow:latest-gpu-py3
      volumeMounts:
      - {mountPath: '{{inputs.parameters.data_path}}', name: create-volume}
    inputs:
      parameters:
      - {name: create-volume-name}
      - {name: data_path}
      - {name: model_file}
    volumes:
    - name: create-volume
      persistentVolumeClaim: {claimName: '{{inputs.parameters.create-volume-name}}'}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--data-path", {"inputValue": "data_path"}, "--model-file", {"inputValue":
          "model_file"}], "command": ["python3", "-u", "-c", "def predict(data_path,
          model_file):\n\n    import pickle\n    import tensorflow as tf\n    from
          tensorflow import keras\n    import numpy as np\n\n    # Load the saved
          Keras model\n    classifier = keras.models.load_model(f''{data_path}/{model_file}'')\n\n    #
          Load and unpack the test_data\n    with open(f''{data_path}/test_data'',''rb'')
          as f:\n        test_data = pickle.load(f)\n    # Separate the X_test from
          y_test.\n    X_test,  y_test = test_data\n\n    # make predictions.\n    y_pred
          = classifier.predict(X_test)\n\n    # create a threshold\n    y_pred=(y_pred>0.5)\n\n    with
          open(f''{data_path}/result.txt'', ''w'') as result:\n        result.write(\"
          Prediction: {}, Actual: {} \".format(y_pred,y_test.astype(np.bool)))\n\n    print(''Prediction
          has be saved successfully!'')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Predict'',
          description='''')\n_parser.add_argument(\"--data-path\", dest=\"data_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-file\",
          dest=\"model_file\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = predict(**_parsed_args)\n"],
          "image": "tensorflow/tensorflow:latest-gpu-py3"}}, "inputs": [{"name": "data_path"},
          {"name": "model_file"}], "name": "Predict"}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: print-prediction
    container:
      args: [cat, '{{inputs.parameters.data_path}}/result.txt']
      image: library/bash:4.4.23
      volumeMounts:
      - {mountPath: '{{inputs.parameters.data_path}}', name: create-volume}
    inputs:
      parameters:
      - {name: create-volume-name}
      - {name: data_path}
    volumes:
    - name: create-volume
      persistentVolumeClaim: {claimName: '{{inputs.parameters.create-volume-name}}'}
  - name: train
    container:
      args: [--data-path, '{{inputs.parameters.data_path}}', --model-file, '{{inputs.parameters.model_file}}']
      command:
      - python3
      - -u
      - -c
      - "def train(data_path, model_file):\n  import pickle\n  import numpy as np\n\
        \  import pandas as pd\n  from sklearn.model_selection import train_test_split\n\
        \  import tensorflow as tf\n\n  data = \"https://raw.githubusercontent.com/Fitzpatrique/stage-f-09-campaign-finance/master/data/new_project_data2.csv\"\
        \n  df = pd.read_csv(data)\n\n  X = df[['can_off_dis', 'can_zip', 'ind_con',\
        \ 'net_ope_exp', 'tot_con',\n       'tot_dis', 'net_con', 'ope_exp', 'tot_rec',\
        \ 'can_off_id', 'can_nam_id',\n       'can_off_sta_id', 'can_par_aff_id',\
        \ 'can_inc_cha_ope_sea_id',\n       'can_cit_id', 'can_sta_id', 'cov_dur']]\n\
        \  y = df[['winner_id']]\n\n  #Perform train test split on the data\n  X_train,\
        \ X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n\
        \n  #Define the model \n  model  = tf.keras.Sequential([\n               \
        \ tf.keras.layers.Flatten(input_shape=(1,17)),\n                tf.keras.layers.Dense(8,\
        \ activation = 'relu'),\n                tf.keras.layers.Dense(1, activation\
        \ = 'sigmoid')\n  ])\n\n  model.compile(optimizer = 'adam', loss='binary_crossentropy',\
        \ metrics =['accuracy'])\n\n  num_epochs = 170\n\n  history = model.fit(X_train,\
        \ y_train, epochs = num_epochs,\n                    validation_data = (X_test,y_test))\n\
        \n  #Save the model to the designated \n  model.save(f'{data_path}/{model_file}')\n\
        \n  #Save the test_data as a pickle file to be used by the predict component.\n\
        \  with open(f'{data_path}/test_data', 'wb') as f:\n      pickle.dump((X_test,\
        \  y_test), f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Train',\
        \ description='')\n_parser.add_argument(\"--data-path\", dest=\"data_path\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --model-file\", dest=\"model_file\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = train(**_parsed_args)\n"
      image: tensorflow/tensorflow:latest-gpu-py3
      volumeMounts:
      - {mountPath: '{{inputs.parameters.data_path}}', name: create-volume}
    inputs:
      parameters:
      - {name: create-volume-name}
      - {name: data_path}
      - {name: model_file}
    volumes:
    - name: create-volume
      persistentVolumeClaim: {claimName: '{{inputs.parameters.create-volume-name}}'}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--data-path", {"inputValue": "data_path"}, "--model-file", {"inputValue":
          "model_file"}], "command": ["python3", "-u", "-c", "def train(data_path,
          model_file):\n  import pickle\n  import numpy as np\n  import pandas as
          pd\n  from sklearn.model_selection import train_test_split\n  import tensorflow
          as tf\n\n  data = \"https://raw.githubusercontent.com/Fitzpatrique/stage-f-09-campaign-finance/master/data/new_project_data2.csv\"\n  df
          = pd.read_csv(data)\n\n  X = df[[''can_off_dis'', ''can_zip'', ''ind_con'',
          ''net_ope_exp'', ''tot_con'',\n       ''tot_dis'', ''net_con'', ''ope_exp'',
          ''tot_rec'', ''can_off_id'', ''can_nam_id'',\n       ''can_off_sta_id'',
          ''can_par_aff_id'', ''can_inc_cha_ope_sea_id'',\n       ''can_cit_id'',
          ''can_sta_id'', ''cov_dur'']]\n  y = df[[''winner_id'']]\n\n  #Perform train
          test split on the data\n  X_train, X_test, y_train, y_test = train_test_split(X,
          y, test_size=0.2, random_state=101)\n\n  #Define the model \n  model  =
          tf.keras.Sequential([\n                tf.keras.layers.Flatten(input_shape=(1,17)),\n                tf.keras.layers.Dense(8,
          activation = ''relu''),\n                tf.keras.layers.Dense(1, activation
          = ''sigmoid'')\n  ])\n\n  model.compile(optimizer = ''adam'', loss=''binary_crossentropy'',
          metrics =[''accuracy''])\n\n  num_epochs = 170\n\n  history = model.fit(X_train,
          y_train, epochs = num_epochs,\n                    validation_data = (X_test,y_test))\n\n  #Save
          the model to the designated \n  model.save(f''{data_path}/{model_file}'')\n\n  #Save
          the test_data as a pickle file to be used by the predict component.\n  with
          open(f''{data_path}/test_data'', ''wb'') as f:\n      pickle.dump((X_test,  y_test),
          f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Train'',
          description='''')\n_parser.add_argument(\"--data-path\", dest=\"data_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-file\",
          dest=\"model_file\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = train(**_parsed_args)\n"], "image":
          "tensorflow/tensorflow:latest-gpu-py3"}}, "inputs": [{"name": "data_path"},
          {"name": "model_file"}], "name": "Train"}', pipelines.kubeflow.org/component_ref: '{}'}
  arguments:
    parameters:
    - {name: data_path}
    - {name: model_file}
  serviceAccountName: pipeline-runner
